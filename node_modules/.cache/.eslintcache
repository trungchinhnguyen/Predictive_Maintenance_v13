[{"/Users/nguyentrungchinh/project_25g/src/index.js":"1","/Users/nguyentrungchinh/project_25g/src/reportWebVitals.js":"2","/Users/nguyentrungchinh/project_25g/src/App.jsx":"3","/Users/nguyentrungchinh/project_25g/src/inference.js":"4","/Users/nguyentrungchinh/project_25g/src/bert_tokenizer.ts":"5"},{"size":500,"mtime":1697546678445,"results":"6","hashOfConfig":"7"},{"size":362,"mtime":1664259239000,"results":"8","hashOfConfig":"7"},{"size":4812,"mtime":1698026890505,"results":"9","hashOfConfig":"7"},{"size":3677,"mtime":1697546682858,"results":"10","hashOfConfig":"7"},{"size":7921,"mtime":1664259239000,"results":"11","hashOfConfig":"7"},{"filePath":"12","messages":"13","suppressedMessages":"14","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},"ln3h6h",{"filePath":"15","messages":"16","suppressedMessages":"17","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"18","messages":"19","suppressedMessages":"20","errorCount":0,"fatalErrorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":"21"},{"filePath":"22","messages":"23","suppressedMessages":"24","errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"25"},{"filePath":"26","messages":"27","suppressedMessages":"28","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},"/Users/nguyentrungchinh/project_25g/src/index.js",[],[],"/Users/nguyentrungchinh/project_25g/src/reportWebVitals.js",[],[],"/Users/nguyentrungchinh/project_25g/src/App.jsx",["29","30","31"],[],"import './App.css';\nimport './App.scss';\n\nimport axios from 'axios';\nimport React from 'react';\nimport { Component } from 'react';\nimport { useState } from 'react';\n\nimport {inference} from './inference.js';\nimport {columnNames} from './inference.js';\nimport {modelDownloadInProgress} from './inference.js';\nimport Chart from \"react-google-charts\";\nimport Box from '@mui/material/Box';\nimport LinearProgress from '@mui/material/LinearProgress';\n\nimport ReactFileReader from 'react-file-reader';\n\n\nclass App extends React.Component {\n  constructor(props) {\n    super(props);\n    this.state = {\n      text: '',\n      data:columnNames,\n      latency:0.0,\n      downloading:modelDownloadInProgress(),\n      selectedFile: null,\n    };\n\n  }\n\n  componentDidMount() {\n\n    this.timerID = setInterval(\n      () => this.checkModelStatus(),\n      1000\n    );\n  }\n\n  checkModelStatus() {\n    this.setState({\n      downloading: modelDownloadInProgress(),\n    });\n    if (!this.state.downloading) {\n      this.timerID = setInterval(\n        () => this.checkModelStatus,\n        5000000\n      );\n    }\n  }\n\n\n  onFileChange =event => {\n    this.setState({ selectedFile: event.target.files[0] });\n          const file = event.target.files[0]; \n          let reader = new FileReader();\n          reader.onload = function(event) {\n            const file = event.target.result;\n            console.log(file);\n            console.log(file.name);\n          };\n\n          reader.onerror = (e) => alert(e.target.error.name);\n          reader.readAsText(file); \n\n  }\n\n\n  onFileUpload=() => {\n\n    axios.post('https://predictivemaintenancev21.onrender.com/upload', {file: (this.state.selectedFile, this.state.selectedFile.name)}\n          )\n          .then((res) => {\n            console.log(res);\n            })\n            .catch((err) => {\n                console.error(err)\n            })\n        let reader = new FileReader();\n\n        reader.onload = () => {\n\n          const lines = reader.result.split(\"\\n\");\n          const headers = lines[0].split(',');\n          const currentLine = lines[1].split(',');\n          const textline = [];\n\n          for (let j = 0; j < headers.length; j++) {\n            if (j==0) {\n              textline[j] = (headers[j] + ' ' + currentLine[j] + '\\n');\n            } \n            else {\n              textline[j] = (textline[j-1]+ headers[j] + ' ' + currentLine[j] + '\\n');\n            }\n          }\n\n          this.setState({\n            text: textline[headers.length-1]\n          })\n\n          inference(textline[headers.length-1]).then( result => {\n            this.setState({\n              text : this.state.text,\n              data:result[1],\n              latency:result[0],\n            });\n          })\n\n        };\n        reader.readAsText(this.state.selectedFile);\n\n     };\n\n\n  render() {\n\n    return (\n      <div className=\"App\">\n\n      <header className=\"App-header\">   \n      <em>Precdictive Maintenance: Transformer Inference</em>\n      <div><font size=\"2\">If predictive result is true, based on uploading data, please check the components to prevent the system's failure in next step</font></div>\n      <Chart  \n        width={'400px'}\n        height={'200px'}\n        chartType=\"BarChart\"\n        data={this.state.data}\n        options={{\n          chartArea: { width: '40%'},\n          colors: ['yellow'],\n          backgroundColor: '#282c34',\n          legend: { \n            textStyle: {color: 'white', fontSize: 10},\n            labels: {fontColor:'white'}\n          },\n          vAxis: {\n            textStyle: {\n            color: 'white',\n            fontSize: 13\n          }\n          },\n          hAxis: {\n            minValue: 5,\n            maxValue: 50,\n            textStyle: {\n              color: 'white'\n            }\n          }\n      }}\n      />\n\n      <div>\n        <h5>File Upload!</h5>\n        <div>\n          <input type=\"file\" onChange={this.onFileChange} />\n          <button onClick={this.onFileUpload}>\n            Upload!\n          </button>\n          <ReactFileReader  handleFiles={this.handleFiles} fileTypes={'.csv'}>\n          </ReactFileReader>\n          <textarea\n                cols={60}\n                rows={20}\n                value={this.state.text}\n                // value={this.state.csvData}\n                onChange={this.onFileChange}\n                style={{ marginTop: 15, width: \"50%\" }}\n            ></textarea> \n        </div>\n      </div>\n\n\n      {this.state.downloading && \n        <div><font size=\"2\">Downloading model from CDN to browser..</font>\n        <Box sx={{ width: '400px' }}>\n        <LinearProgress />\n        </Box> \n        <p></p>\n        </div>\n      }\n\n       <div><font size=\"4\">Inference Latency {this.state.latency} ms</font></div>\n  \n      </header> \n\n    </div>   \n    );\n\n\n  };\n  \n\n}\nexport default App;\n\n","/Users/nguyentrungchinh/project_25g/src/inference.js",["32"],[],"/** */\n/*global BigInt */\n/*global BigInt64Array */\n\nimport { loadTokenizer } from './bert_tokenizer.ts';\nimport * as wasmFeatureDetect from 'wasm-feature-detect';\n\n//Setup onnxruntime \nconst ort = require('onnxruntime-web');\n\n//requires Cross-Origin-*-policy headers https://web.dev/coop-coep/\n/**\nconst simdResolver = wasmFeatureDetect.simd().then(simdSupported => {\n    console.log(\"simd is supported? \"+ simdSupported);\n    if (simdSupported) {\n      ort.env.wasm.numThreads = 3; \n      ort.env.wasm.simd = true;\n    } else {\n      ort.env.wasm.numThreads = 1; \n      ort.env.wasm.simd = false;\n    }\n});\n*/\n\nconst options = {\n  executionProviders: ['wasm'], \n  graphOptimizationLevel: 'all'\n};\n\nvar downLoadingModel = true;\nconst model = \"./classifier_2_int8.onnx\";\n\nconst session = ort.InferenceSession.create(model, options);\nsession.then(t => { \n  downLoadingModel = false;\n  //warmup the VM\n  for(var i = 0; i < 10; i++) {\n    console.log(\"Inference warmup \" + i);\n    lm_inference(\"this is a warmup inference\");\n  }\n});\n\nconst tokenizer = loadTokenizer()\n\nconst STATUS_DEFAULT_DISPLAY = [\n  [\"Status\", \"Score\"],\n  ['false: normal system next step ðŸ‘',0],\n  ['ttrue: fail system next step ðŸ‘Ž',0],\n];\n\nconst STATUS = [\n  'false: normal system next step ðŸ‘',\n  'true: fail system next step ðŸ‘Ž'\n];\n\nfunction isDownloading() {\n  return downLoadingModel;\n}\n\nfunction sortResult(a, b) {\n  if (a[1] === b[1]) {\n      return 0;\n  }\n  else {\n      return (a[1] < b[1]) ? 1 : -1;\n  }\n}\n\nfunction sigmoid(t) {\n  return 1/(1+Math.pow(Math.E, -t));\n}\n\nfunction create_model_input(encoded) {\n  var input_ids = new Array(encoded.length+2);\n  var attention_mask = new Array(encoded.length+2);\n  var token_type_ids = new Array(encoded.length+2);\n  input_ids[0]Â = BigInt(101);\n  attention_mask[0]Â = BigInt(1);\n  token_type_ids[0]Â = BigInt(0);\n  var i = 0;\n  for(; i < encoded.length; i++) { \n    input_ids[i+1] = BigInt(encoded[i]);\n    attention_mask[i+1] = BigInt(1);\n    token_type_ids[i+1] = BigInt(0);\n  }\n  input_ids[i+1]Â = BigInt(102);\n  attention_mask[i+1]Â = BigInt(1);\n  token_type_ids[i+1]Â = BigInt(0);\n  const sequence_length = input_ids.length;\n  input_ids = new ort.Tensor('int64', BigInt64Array.from(input_ids), [1,sequence_length]);\n  attention_mask = new ort.Tensor('int64', BigInt64Array.from(attention_mask), [1,sequence_length]);\n  token_type_ids = new ort.Tensor('int64', BigInt64Array.from(token_type_ids), [1,sequence_length]);\n  return {\n    input_ids: input_ids,\n    attention_mask: attention_mask,\n    token_type_ids:token_type_ids\n  }\n}\n\nasync function lm_inference(text) {\n  try { \n    const encoded_ids = await tokenizer.then(t => {\n      return t.tokenize(text); \n    });\n    if(encoded_ids.length === 0) {\n      return [0.0, STATUS_DEFAULT_DISPLAY];\n    }\n    const start = performance.now();\n    const model_input = create_model_input(encoded_ids);\n    const output =  await session.then(s => { return s.run(model_input,['output_0'])});\n    const duration = (performance.now() - start).toFixed(1);\n    const probs = output['output_0'].data.map(sigmoid).map( t => Math.floor(t*100));\n    \n    const result = [];\n    for(var i = 0; i < STATUS.length;i++) {\n      const t = [STATUS[i], probs[i]];\n      result[i] = t;\n    }\n    result.sort(sortResult); \n    \n    const result_list = [];\n    result_list[0] = [\"Status\", \"Score\"];\n    for(i = 0; i < 2; i++) {\n       result_list[i+1] = result[i];\n    }\n    return [duration,result_list];    \n  } catch (e) {\n    return [0.0,STATUS_DEFAULT_DISPLAY];\n  }\n}    \n\nexport let inference = lm_inference \nexport let columnNames = STATUS_DEFAULT_DISPLAY\nexport let modelDownloadInProgress = isDownloading\n","/Users/nguyentrungchinh/project_25g/src/bert_tokenizer.ts",[],[],{"ruleId":"33","severity":1,"message":"34","line":6,"column":10,"nodeType":"35","messageId":"36","endLine":6,"endColumn":19},{"ruleId":"33","severity":1,"message":"37","line":7,"column":10,"nodeType":"35","messageId":"36","endLine":7,"endColumn":18},{"ruleId":"38","severity":1,"message":"39","line":89,"column":18,"nodeType":"40","messageId":"41","endLine":89,"endColumn":20},{"ruleId":"33","severity":1,"message":"42","line":6,"column":13,"nodeType":"35","messageId":"36","endLine":6,"endColumn":30},"no-unused-vars","'Component' is defined but never used.","Identifier","unusedVar","'useState' is defined but never used.","eqeqeq","Expected '===' and instead saw '=='.","BinaryExpression","unexpected","'wasmFeatureDetect' is defined but never used."]